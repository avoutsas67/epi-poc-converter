{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\psaga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import sys\n",
    "from bs4 import NavigableString, BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import string\n",
    "\n",
    "from utils.config import config\n",
    "from utils.logger.logger import loggerCreator\n",
    "\n",
    "# ePI Modules\n",
    "from parse.rulebook.rulebook import StyleRulesDictionary\n",
    "\n",
    "from parse.extractor.parser import parserExtractor\n",
    "from match.matchDocument.matchDocument import MatchDocument\n",
    "from documentAnnotation.documentAnnotation import DocumentAnnotation\n",
    "from htmlDocTypePartitioner.partition import DocTypePartitioner\n",
    "from extractContentBetweenHeadings.dataBetweenHeadingsExtractor import DataBetweenHeadingsExtractor\n",
    "from fhirXmlGenerator.fhirXmlGenerator import FhirXmlGenerator\n",
    "from fhirService.fhirService import FhirService\n",
    "from utils.logger.matchLogger import MatchLogger\n",
    "from languageInfo.documentTypeNames.documentTypeNames import DocumentTypeNames\n",
    "from wordToHtmlConvertor.wordToHtmlConvertor import WordToHtmlConvertor\n",
    "\n",
    "\n",
    "class FolderNotFoundError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def getRandomString(N):\n",
    "    str_ = ''.join(random.choice(string.ascii_uppercase + string.digits\n",
    "                                 + string.ascii_lowercase) for _ in range(N))\n",
    "    return str_\n",
    "\n",
    "\n",
    "def convertHtmlToJson(controlBasePath, basePath, domain, procedureType, languageCode, htmlDocName, fileNameQrd, fileNameLog):\n",
    "\n",
    "    module_path = os.path.join(basePath)\n",
    "\n",
    "    if \"/\" in basePath:\n",
    "        pathSep = \"/\"\n",
    "    else:\n",
    "        pathSep = \"\\\\\"\n",
    "    \n",
    "    # Generate output folder path\n",
    "    output_json_path = os.path.join(basePath, 'outputJSON')\n",
    "\n",
    "    \"\"\"\n",
    "        Check if input folder exists, else throw exception\n",
    "    \"\"\"\n",
    "    if(os.path.exists(module_path)):\n",
    "        filenames = glob.glob(os.path.join(module_path, htmlDocName))\n",
    "\n",
    "        # Create language specific folder in outputJSON folder if it doesn't exist\n",
    "        if(not os.path.exists(output_json_path)):\n",
    "            os.mkdir(output_json_path)\n",
    "        logger = MatchLogger(f'Parser_{getRandomString(1)}', htmlDocName,\n",
    "                             domain, procedureType, languageCode, \"HTML\", fileNameLog)\n",
    "\n",
    "        styleLogger = MatchLogger(\n",
    "            f'Style Dictionary_{getRandomString(1)}', htmlDocName, domain, procedureType, languageCode, \"HTML\", fileNameLog)\n",
    "\n",
    "        styleRulesObj = StyleRulesDictionary(logger=styleLogger,\n",
    "                                             controlBasePath=controlBasePath,\n",
    "                                             language=languageCode,\n",
    "                                             fileName=fileNameQrd,\n",
    "                                             domain=domain,\n",
    "                                             procedureType=procedureType\n",
    "                                             )\n",
    "\n",
    "        parserObj = parserExtractor(config, logger, styleRulesObj.styleRuleDict,\n",
    "                                    styleRulesObj.styleFeatureKeyList,\n",
    "                                    styleRulesObj.qrd_section_headings)\n",
    "\n",
    "        for input_filename in filenames:\n",
    "          # if(input_filename.find('Kalydeco II-86-PI-clean')!=-1):\n",
    "            output_filename = os.path.join(output_json_path, htmlDocName)\n",
    "            style_filepath =  output_filename.replace('.html','.txt')\n",
    "            style_filepath =  style_filepath.replace('.txtl','.txt')\n",
    "            style_filepath =  style_filepath.replace('.htm','.txt')\n",
    "            print(\"-------------\",style_filepath,\"-----------------\")\n",
    "\n",
    "            output_filename = output_filename.replace('.html', '.json')\n",
    "            output_filename = output_filename.replace('.htm', '.json')\n",
    "            print(input_filename, output_filename)\n",
    "            parserObj.createPIJsonFromHTML(input_filepath=input_filename,\n",
    "                                           output_filepath=output_filename,\n",
    "                                           style_filepath = style_filepath,\n",
    "                                           img_base64_dict=parserObj.convertImgToBase64(input_filename)\n",
    "                                           )\n",
    "            \n",
    "        return output_filename.split(pathSep)[-1], style_filepath\n",
    "    else:\n",
    "        try:    \n",
    "            raise FolderNotFoundError(module_path + \" not found\")\n",
    "        except:  \n",
    "            logger.logFlowCheckpoint(\"Folder For Language Code Not Found In Input File\")\n",
    "            logger.logException(\"Folder For Language Code Not Found In Input File\")\n",
    "        raise FolderNotFoundError(module_path + \" not found\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def splitJson(controlBasePath, basePath, domain, procedureType, languageCode, fileNameJson, fileNameQrd, fileNameLog):\n",
    "\n",
    "    styleLogger = MatchLogger(\n",
    "        f'Style Dictionary_{getRandomString(1)}', fileNameJson, domain, procedureType, languageCode, \"Json\", fileNameLog)\n",
    "\n",
    "    styleRulesObj = StyleRulesDictionary(logger=styleLogger,\n",
    "                                        controlBasePath=controlBasePath,\n",
    "                                        language=languageCode,\n",
    "                                        fileName=fileNameQrd,\n",
    "                                        domain=domain,\n",
    "                                        procedureType=procedureType\n",
    "                                        )\n",
    "    \n",
    "    path_json = os.path.join(basePath,'outputJSON', fileNameJson)\n",
    "    print(\"PathJson\",path_json)\n",
    "    partitionLogger = MatchLogger(\n",
    "        f'Partition_{getRandomString(1)}', fileNameJson, domain, procedureType, languageCode, \"Json\", fileNameLog)\n",
    "\n",
    "    partitioner = DocTypePartitioner(partitionLogger)\n",
    "\n",
    "    partitionedJsonPaths = partitioner.partitionHtmls(\n",
    "        styleRulesObj.qrd_section_headings, path_json)\n",
    "\n",
    "    return partitionedJsonPaths\n",
    "\n",
    "\n",
    "def extractAndValidateHeadings(controlBasePath,\n",
    "                                basePath,\n",
    "                                domain,\n",
    "                                procedureType,\n",
    "                                languageCode,\n",
    "                                documentNumber,\n",
    "                                fileNameDoc,\n",
    "                                fileNameQrd,\n",
    "                                fileNameMatchRuleBook,\n",
    "                                fileNameDocumentTypeNames,\n",
    "                                fileNameLog,\n",
    "                                stopWordFilterLen=6,\n",
    "                                isPackageLeaflet=False,\n",
    "                                medName=None\n",
    "                                ):\n",
    "\n",
    "    if documentNumber == 0:\n",
    "        topHeadingsConsidered = 4\n",
    "        bottomHeadingsConsidered = 6\n",
    "    elif documentNumber == 1:\n",
    "        topHeadingsConsidered = 3\n",
    "        bottomHeadingsConsidered = 5\n",
    "    elif documentNumber == 2:\n",
    "        topHeadingsConsidered = 5\n",
    "        bottomHeadingsConsidered = 15\n",
    "    else:\n",
    "        topHeadingsConsidered = 5\n",
    "        bottomHeadingsConsidered = 10\n",
    "\n",
    "    print(f\"Starting Heading Extraction For File :- {fileNameDoc}\")\n",
    "    logger = MatchLogger(f\"Heading Extraction {fileNameDoc}_{getRandomString(1)}\", fileNameDoc, domain, procedureType, languageCode, documentNumber, fileNameLog)\n",
    "    logger.logFlowCheckpoint(\"Starting Heading Extraction\")\n",
    "\n",
    "    stopWordlanguage = DocumentTypeNames(\n",
    "        controlBasePath=controlBasePath,\n",
    "        fileNameDocumentTypeNames=fileNameDocumentTypeNames,\n",
    "        languageCode=languageCode,\n",
    "        domain=domain,\n",
    "        procedureType=procedureType,\n",
    "        documentNumber=documentNumber\n",
    "        ).extractStopWordLanguage()\n",
    "\n",
    "    matchDocObj = MatchDocument(\n",
    "        logger,\n",
    "        controlBasePath,\n",
    "        basePath,\n",
    "        domain,\n",
    "        procedureType,\n",
    "        languageCode,\n",
    "        documentNumber,\n",
    "        fileNameDoc,\n",
    "        fileNameQrd,\n",
    "        fileNameMatchRuleBook,\n",
    "        fileNameDocumentTypeNames,\n",
    "        topHeadingsConsidered,\n",
    "        bottomHeadingsConsidered,\n",
    "        stopWordFilterLen,\n",
    "        stopWordlanguage,\n",
    "        isPackageLeaflet,\n",
    "        medName)\n",
    "    df, coll = matchDocObj.matchHtmlHeaddingsWithQrd()\n",
    "\n",
    "    return df, coll\n",
    "\n",
    "\n",
    "def parseDocument(controlBasePath, basePath ,htmlDocName, fileNameQrd, fileNameMatchRuleBook, fileNameDocumentTypeNames, medName = None):\n",
    "    \n",
    "    if \"/\" in basePath:\n",
    "        pathSep = \"/\"        \n",
    "    else:\n",
    "        pathSep = \"\\\\\"\n",
    "    \n",
    "    fileNameLog = os.path.join(basePath,'FinalLog.txt')\n",
    "\n",
    "    pathComponents = basePath.split(pathSep)\n",
    "    print(pathComponents, htmlDocName)\n",
    "    timestamp = pathComponents[-1]\n",
    "    languageCode =  pathComponents[-2]\n",
    "    medName = pathComponents[-3]\n",
    "    procedureType = pathComponents[-4]\n",
    "    domain = pathComponents[-5]\n",
    "\n",
    "    print(timestamp, languageCode, medName, procedureType, domain)\n",
    "        \n",
    "    flowLogger =  MatchLogger(f\"Flow Logger HTML_{getRandomString(1)}\", htmlDocName, domain, procedureType, languageCode, \"HTML\", fileNameLog)\n",
    "\n",
    "    flowLogger.logFlowCheckpoint(\"Starting HTML Conversion To Json\")\n",
    "    ###Convert Html to Json\n",
    "    fileNameJson, stylesFilePath = convertHtmlToJson(controlBasePath, basePath, domain, procedureType, languageCode, htmlDocName, fileNameQrd, fileNameLog)\n",
    "    \n",
    "    print(\"stylePath:-\",stylesFilePath)\n",
    "    flowLogger.logFlowCheckpoint(\"Completed HTML Conversion To Json\")\n",
    "\n",
    "    flowLogger.logFlowCheckpoint(\"Starting Json Split\")\n",
    "\n",
    "    ###Split Uber Json to multiple Jsons for each category.\n",
    "    partitionedJsonPaths = splitJson(controlBasePath, basePath, domain, procedureType, languageCode, fileNameJson, fileNameQrd, fileNameLog)\n",
    "    \n",
    "    partitionedJsonPaths = [ path.split(pathSep)[-1] for path in partitionedJsonPaths]\n",
    "    flowLogger.logFlowCheckpoint(str(partitionedJsonPaths))\n",
    "    \n",
    "    flowLogger.logFlowCheckpoint(\"Completed Json Split\")\n",
    "    \n",
    "    flowLogger.logFlowCheckpoint(\"Started Processing Partitioned Jsons\")\n",
    "    \n",
    "    for index, fileNamePartitioned in enumerate(partitionedJsonPaths):\n",
    "        \n",
    "        flowLogger.logFlowCheckpoint(f\"\\n\\n\\n\\n||||||||||||||||||||||||||||||||{str(index)} ||||| {str(fileNamePartitioned)}||||||||||||||||||||||||||||||||\\n\\n\\n\\n\")\n",
    "        \n",
    "        if index == 3:\n",
    "            stopWordFilterLen = 100\n",
    "            isPackageLeaflet = True\n",
    "        else:\n",
    "            stopWordFilterLen = 6\n",
    "            isPackageLeaflet = False\n",
    "            \n",
    "        df, coll = extractAndValidateHeadings(controlBasePath,\n",
    "                                    basePath,\n",
    "                                    domain,\n",
    "                                    procedureType,\n",
    "                                    languageCode,\n",
    "                                    index,\n",
    "                                    fileNamePartitioned,\n",
    "                                    fileNameQrd,\n",
    "                                    fileNameMatchRuleBook,\n",
    "                                    fileNameDocumentTypeNames,\n",
    "                                    fileNameLog,\n",
    "                                    stopWordFilterLen=stopWordFilterLen,\n",
    "                                    isPackageLeaflet=isPackageLeaflet,\n",
    "                                    medName=medName)\n",
    "        \n",
    "        \n",
    "        print(f\"Completed Heading Extraction For File\")\n",
    "        flowLogger.logFlowCheckpoint(\"Completed Heading Extraction For File\")\n",
    "        \n",
    "        print(f\"Starting Document Annotation For File :- {fileNamePartitioned}\")        \n",
    "        flowLogger.logFlowCheckpoint(\"Starting Document Annotation For File\")\n",
    "        documentAnnotationObj = DocumentAnnotation(fileNamePartitioned,'c20835db4b1b4e108828a8537ff41506','https://spor-sit.azure-api.net/pms/api/v2/',df,coll)\n",
    "        try:\n",
    "            pms_oms_annotation_data = documentAnnotationObj.processRegulatedAuthorizationForDoc()\n",
    "            print(pms_oms_annotation_data)\n",
    "        except:\n",
    "            pms_oms_annotation_data = None\n",
    "            print(\"Error Found\")\n",
    "            \n",
    "        print(f\"Completed Document Annotation\")        \n",
    "        flowLogger.logFlowCheckpoint(\"Completed Document Annotation\")\n",
    "        \n",
    "        print(f\"Starting Extracting Content Between Heading For File :- {fileNamePartitioned}\")        \n",
    "        flowLogger.logFlowCheckpoint(\"Starting Extracting Content Between Heading\")\n",
    "        \n",
    "        extractContentlogger =  MatchLogger(f'ExtractContentBetween_{index}_{getRandomString(1)}', fileNamePartitioned, domain, procedureType, languageCode, index, fileNameLog)\n",
    "        extractorObj = DataBetweenHeadingsExtractor(extractContentlogger, basePath, coll)\n",
    "        dfExtractedHierRR = extractorObj.extractContentBetweenHeadings(fileNamePartitioned)\n",
    "        \n",
    "        print(f\"Completed Extracting Content Between Heading\")        \n",
    "        flowLogger.logFlowCheckpoint(\"Completed Extracting Content Between Heading\")\n",
    "        \n",
    "        xmlLogger =  MatchLogger(f'XmlGeneration_{index}_{getRandomString(1)}', fileNamePartitioned, domain, procedureType, languageCode, index, fileNameLog)\n",
    "        fhirXmlGeneratorObj = FhirXmlGenerator(xmlLogger, controlBasePath, basePath, pms_oms_annotation_data, stylesFilePath, medName)\n",
    "        fileNameXml = fileNamePartitioned.replace('.json','.xml')\n",
    "        generatedXml = fhirXmlGeneratorObj.generateXml(dfExtractedHierRR, fileNameXml)\n",
    "        \n",
    "        fhirServiceLogger =  MatchLogger(f'XML Submission Logger_{index}_{getRandomString(1)}', fileNamePartitioned, domain, procedureType, languageCode, index, fileNameLog)\n",
    "\n",
    "        fhirServiceObj = FhirService(fhirServiceLogger, basePath, generatedXml)\n",
    "        fhirServiceObj.submitFhirXml()\n",
    "        print(f\"Created XML File For :- {fileNamePartitioned}\")      \n",
    "\n",
    "        #return df,coll,dfExtractedHierRR\n",
    "    \n",
    "    flowLogger.logFlowCheckpoint(\"Completed Processing Partitioned Jsons\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 20:50:08,853 : WordToHtmlLoggerS : Input file: C:\\Users\\psaga\\source\\repos\\EMA\\EMA%20EPI%20PoC\\function_code\\data\\Ingest\\ABASAGLAR~H~CAP~en.docx | H | CAP |  en | .docx | ABASAGLAR~H~CAP~en.docx\n",
      "2021-05-13 20:50:08,856 : WordToHtmlLoggerS : Output file: C:\\Users\\psaga\\source\\repos\\EMA\\EMA%20EPI%20PoC\\function_code\\work\\H\\CAP\\ABASAGLAR\\en\\2021-05-13T15-20-08Z\\ABASAGLAR_clean | H | CAP |  en | .docx | ABASAGLAR~H~CAP~en.docx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Files in folder:  ['ABASAGLAR~H~CAP~en.docx', 'Abilify Maintena~H~CAP~en.doc', '~$ilify Maintena~H~CAP~en.doc']\n",
      "Input file: C:\\Users\\psaga\\source\\repos\\EMA\\EMA%20EPI%20PoC\\function_code\\data\\Ingest\\ABASAGLAR~H~CAP~en.docx\n",
      "('ABASAGLAR~H~CAP~en', '.doc', 'x')\n",
      "ABASAGLAR~H~CAP~en.docx\n",
      "Output file: C:\\Users\\psaga\\source\\repos\\EMA\\EMA%20EPI%20PoC\\function_code\\work\\H\\CAP\\ABASAGLAR\\en\\2021-05-13T15-20-08Z\\ABASAGLAR_clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 20:50:12,704 : WordToHtmlLoggerS : Opened file: C:\\Users\\psaga\\source\\repos\\EMA\\EMA%20EPI%20PoC\\function_code\\data\\Ingest\\ABASAGLAR~H~CAP~en.docx | H | CAP |  en | .docx | ABASAGLAR~H~CAP~en.docx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened file: C:\\Users\\psaga\\source\\repos\\EMA\\EMA%20EPI%20PoC\\function_code\\data\\Ingest\\ABASAGLAR~H~CAP~en.docx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 20:50:13,899 : WordToHtmlLoggerS : Starting document cleaning process | H | CAP |  en | .docx | ABASAGLAR~H~CAP~en.docx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking table 20\n",
      "The selection starts on page 105 of 106 (69.75/71.0)\n",
      "The selection ends on page 105 of 106 (384.75/71.0)\n",
      "The selection contains\n",
      "* overlay images\n",
      "\n",
      "Checking table 19\n",
      "The selection starts on page 103 of 106 (579.5/71.0)\n",
      "The selection ends on page 105 of 106 (56.75/71.0)\n",
      "The selection contains\n",
      "* overlay images\n",
      "\n",
      "Checking table 18\n",
      "The selection starts on page 102 of 105 (564.75/215.0)\n",
      "The selection ends on page 103 of 105 (421.75/71.0)\n",
      "The selection contains\n",
      "* overlay images\n",
      "* overlay shapes\n",
      "\n",
      "Checking table 17\n",
      "The selection starts on page 102 of 105 (56.75/71.0)\n",
      "The selection ends on page 102 of 105 (476.25/71.0)\n",
      "The selection contains\n",
      "* overlay images\n",
      "\n",
      "Checking table 16\n",
      "The selection starts on page 100 of 105 (578.0/71.0)\n",
      "The selection ends on page 101 of 105 (484.5/71.0)\n",
      "The selection contains\n",
      "* overlay images\n",
      "\n",
      "Checking table 15\n",
      "The selection starts on page 100 of 105 (196.75/71.0)\n",
      "The selection ends on page 100 of 105 (331.75/71.0)\n",
      "The selection contains\n",
      "* overlay images\n",
      "\n",
      "Checking table 14\n",
      "The selection starts on page 100 of 105 (56.75/71.0)\n",
      "The selection ends on page 100 of 105 (196.75/71.0)\n",
      "The selection contains\n",
      "* inline images\n",
      "\n",
      "Checking table 13\n",
      "The selection starts on page 94 of 105 (613.25/71.0)\n",
      "The selection ends on page 95 of 105 (663.75/71.0)\n",
      "The selection contains\n",
      "\n",
      "Checking table 12\n",
      "The selection starts on page 86 of 105 (69.25/71.0)\n",
      "The selection ends on page 86 of 105 (416.5/71.0)\n",
      "The selection contains\n",
      "* overlay images\n",
      "\n",
      "Checking table 11\n",
      "The selection starts on page 84 of 105 (632.0/71.0)\n",
      "The selection ends on page 85 of 105 (745.0/71.0)\n",
      "The selection contains\n",
      "* overlay images\n",
      "* overlay shapes\n",
      "\n",
      "Checking table 10\n",
      "The selection starts on page 83 of 105 (767.75/71.0)\n",
      "The selection ends on page 84 of 105 (475.75/71.0)\n",
      "The selection contains\n",
      "* overlay images\n",
      "* overlay shapes\n",
      "\n",
      "Checking table 9\n",
      "The selection starts on page 83 of 105 (237.75/71.0)\n",
      "The selection ends on page 83 of 105 (660.0/71.0)\n",
      "The selection contains\n",
      "* overlay images\n",
      "\n",
      "Checking table 8\n",
      "The selection starts on page 82 of 105 (406.5/71.0)\n",
      "The selection ends on page 83 of 105 (153.75/71.0)\n",
      "The selection contains\n",
      "* overlay images\n",
      "* overlay shapes\n",
      "\n",
      "Checking table 7\n",
      "The selection starts on page 81 of 106 (667.75/71.0)\n",
      "The selection ends on page 82 of 106 (138.25/71.0)\n",
      "The selection contains\n",
      "* inline images\n",
      "\n",
      "Checking table 6\n",
      "The selection starts on page 81 of 106 (545.25/71.0)\n",
      "The selection ends on page 81 of 106 (667.75/71.0)\n",
      "The selection contains\n",
      "* inline images\n",
      "\n",
      "Checking table 5\n",
      "The selection starts on page 81 of 106 (113.0/71.0)\n",
      "The selection ends on page 81 of 106 (195.0/71.0)\n",
      "The selection contains\n",
      "* inline images\n",
      "\n",
      "Checking table 4\n",
      "The selection starts on page 76 of 106 (524.75/71.0)\n",
      "The selection ends on page 77 of 106 (575.25/71.0)\n",
      "The selection contains\n",
      "\n",
      "Checking table 3\n",
      "The selection starts on page 64 of 106 (639.0/71.0)\n",
      "The selection ends on page 66 of 106 (107.25/71.0)\n",
      "The selection contains\n",
      "\n",
      "Checking table 2\n",
      "The selection starts on page 22 of 106 (158.0/71.0)\n",
      "The selection ends on page 22 of 106 (503.5/71.0)\n",
      "The selection contains\n",
      "\n",
      "Checking table 1\n",
      "The selection starts on page 7 of 106 (512.0/71.0)\n",
      "The selection ends on page 8 of 106 (358.75/71.0)\n",
      "The selection contains\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 20:51:08,369 : WordToHtmlLoggerS : Completed document cleaning process | H | CAP |  en | .docx | ABASAGLAR~H~CAP~en.docx\n",
      "2021-05-13 20:51:08,371 : WordToHtmlLoggerS : Preparing zip file | H | CAP |  en | .docx | ABASAGLAR~H~CAP~en.docx\n",
      "2021-05-13 20:51:10,179 : WordToHtmlLoggerS : Zip file created: C:\\Users\\psaga\\source\\repos\\EMA\\EMA%20EPI%20PoC\\function_code\\inputblob\\ABASAGLAR~H~CAP~en~2021-05-13T15-20-08Z.zip | H | CAP |  en | .docx | ABASAGLAR~H~CAP~en.docx\n",
      "2021-05-13 20:51:10,183 : WordToHtmlLoggerS : Uploading to Azure Storage as blob:\n",
      "\tC:\\Users\\psaga\\source\\repos\\EMA\\EMA%20EPI%20PoC\\function_code\\inputblob\\ABASAGLAR~H~CAP~en~2021-05-13T15-20-08Z.zip | H | CAP |  en | .docx | ABASAGLAR~H~CAP~en.docx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading to Azure Storage as blob:\n",
      "\tC:\\Users\\psaga\\source\\repos\\EMA\\EMA%20EPI%20PoC\\function_code\\inputblob\\ABASAGLAR~H~CAP~en~2021-05-13T15-20-08Z.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 20:51:14,440 : WordToHtmlLoggerS : UploadedC:\\Users\\psaga\\source\\repos\\EMA\\EMA%20EPI%20PoC\\function_code\\inputblob\\ABASAGLAR~H~CAP~en~2021-05-13T15-20-08Z.zipsuccessfully | H | CAP |  en | .docx | ABASAGLAR~H~CAP~en.docx\n",
      "2021-05-13 20:51:14,443 : WordToHtmlLoggerS : Deleting input word file: ABASAGLAR~H~CAP~en.docx | H | CAP |  en | .docx | ABASAGLAR~H~CAP~en.docx\n",
      "2021-05-13 20:51:14,482 : WordToHtmlLoggerJ : Input file: C:\\Users\\psaga\\source\\repos\\EMA\\EMA%20EPI%20PoC\\function_code\\data\\Ingest\\Abilify Maintena~H~CAP~en.doc | H | CAP |  en | .doc | Abilify Maintena~H~CAP~en.doc\n",
      "2021-05-13 20:51:14,484 : WordToHtmlLoggerJ : Output file: C:\\Users\\psaga\\source\\repos\\EMA\\EMA%20EPI%20PoC\\function_code\\work\\H\\CAP\\Abilify Maintena\\en\\2021-05-13T15-21-14Z\\Abilify Maintena_clean | H | CAP |  en | .doc | Abilify Maintena~H~CAP~en.doc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file: C:\\Users\\psaga\\source\\repos\\EMA\\EMA%20EPI%20PoC\\function_code\\data\\Ingest\\Abilify Maintena~H~CAP~en.doc\n",
      "('Abilify Maintena~H~CAP~en', '.doc', '')\n",
      "Abilify Maintena~H~CAP~en.doc\n",
      "Output file: C:\\Users\\psaga\\source\\repos\\EMA\\EMA%20EPI%20PoC\\function_code\\work\\H\\CAP\\Abilify Maintena\\en\\2021-05-13T15-21-14Z\\Abilify Maintena_clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 20:51:16,606 : WordToHtmlLoggerJ : Opened file: C:\\Users\\psaga\\source\\repos\\EMA\\EMA%20EPI%20PoC\\function_code\\data\\Ingest\\Abilify Maintena~H~CAP~en.doc | H | CAP |  en | .doc | Abilify Maintena~H~CAP~en.doc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened file: C:\\Users\\psaga\\source\\repos\\EMA\\EMA%20EPI%20PoC\\function_code\\data\\Ingest\\Abilify Maintena~H~CAP~en.doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 20:51:18,264 : WordToHtmlLoggerJ : Starting document cleaning process | H | CAP |  en | .doc | Abilify Maintena~H~CAP~en.doc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking table 23\n",
      "The selection starts on page 95 of 95 (619.25/71.0)\n",
      "The selection ends on page 95 of 95 (737.0/71.0)\n",
      "The selection contains\n",
      "* inline images\n",
      "\n",
      "Checking table 22\n",
      "The selection starts on page 95 of 95 (303.5/71.0)\n",
      "The selection ends on page 95 of 95 (480.25/71.0)\n",
      "The selection contains\n",
      "* inline images\n",
      "\n",
      "Checking table 21\n",
      "The selection starts on page 95 of 95 (107.25/71.0)\n",
      "The selection ends on page 95 of 95 (227.5/71.0)\n",
      "The selection contains\n",
      "* inline images\n",
      "\n",
      "Checking table 20\n",
      "The selection starts on page 94 of 95 (652.75/71.0)\n",
      "The selection ends on page 95 of 95 (56.75/71.0)\n",
      "The selection contains\n",
      "* inline images\n",
      "\n",
      "Checking table 19\n",
      "The selection starts on page 94 of 96 (494.25/71.0)\n",
      "The selection ends on page 94 of 96 (627.5/71.0)\n",
      "The selection contains\n",
      "* inline images\n",
      "\n",
      "Checking table 18\n",
      "The selection starts on page 94 of 96 (353.25/71.0)\n",
      "The selection ends on page 94 of 96 (456.5/71.0)\n",
      "The selection contains\n",
      "\n",
      "Checking table 17\n",
      "The selection starts on page 94 of 96 (145.25/71.0)\n",
      "The selection ends on page 94 of 96 (315.25/71.0)\n",
      "The selection contains\n",
      "* inline images\n",
      "\n",
      "Checking table 16\n",
      "The selection starts on page 93 of 96 (581.75/71.0)\n",
      "The selection ends on page 93 of 96 (726.25/71.0)\n",
      "The selection contains\n",
      "* inline images\n",
      "\n",
      "Checking table 15\n",
      "The selection starts on page 93 of 96 (398.25/71.0)\n",
      "The selection ends on page 93 of 96 (543.75/71.0)\n",
      "The selection contains\n",
      "* inline images\n",
      "\n",
      "Checking table 14\n",
      "The selection starts on page 91 of 96 (436.25/71.0)\n",
      "The selection ends on page 92 of 96 (499.5/71.0)\n",
      "The selection contains\n",
      "\n",
      "Checking table 13\n",
      "The selection starts on page 82 of 96 (484.75/71.0)\n",
      "The selection ends on page 82 of 96 (582.5/71.0)\n",
      "The selection contains\n",
      "* inline images\n",
      "\n",
      "Checking table 12\n",
      "The selection starts on page 82 of 96 (184.25/71.0)\n",
      "The selection ends on page 82 of 96 (345.5/71.0)\n",
      "The selection contains\n",
      "* inline images\n",
      "\n",
      "Checking table 11\n",
      "The selection starts on page 81 of 96 (697.5/71.0)\n",
      "The selection ends on page 82 of 96 (108.25/71.0)\n",
      "The selection contains\n",
      "\n",
      "Checking table 10\n",
      "The selection starts on page 81 of 96 (286.5/71.0)\n",
      "The selection ends on page 81 of 96 (378.5/71.0)\n",
      "The selection contains\n",
      "\n",
      "Checking table 9\n",
      "The selection starts on page 81 of 96 (194.5/71.0)\n",
      "The selection ends on page 81 of 96 (286.5/71.0)\n",
      "The selection contains\n",
      "\n",
      "Checking table 8\n",
      "The selection starts on page 80 of 96 (235.5/71.0)\n",
      "The selection ends on page 80 of 96 (333.25/71.0)\n",
      "The selection contains\n",
      "* inline images\n",
      "\n",
      "Checking table 7\n",
      "The selection starts on page 79 of 96 (524.75/71.0)\n",
      "The selection ends on page 79 of 96 (658.0/71.0)\n",
      "The selection contains\n",
      "* inline images\n",
      "\n",
      "Checking table 6\n",
      "The selection starts on page 77 of 96 (410.75/71.0)\n",
      "The selection ends on page 78 of 96 (474.0/71.0)\n",
      "The selection contains\n",
      "\n",
      "Checking table 5\n",
      "The selection starts on page 16 of 96 (461.5/71.0)\n",
      "The selection ends on page 16 of 96 (604.0/71.0)\n",
      "The selection contains\n",
      "\n",
      "Checking table 4\n",
      "The selection starts on page 15 of 96 (519.25/71.0)\n",
      "The selection ends on page 15 of 96 (659.5/71.0)\n",
      "The selection contains\n",
      "\n",
      "Checking table 3\n",
      "The selection starts on page 9 of 96 (512.0/71.0)\n",
      "The selection ends on page 12 of 96 (186.75/71.0)\n",
      "The selection contains\n",
      "\n",
      "Checking table 2\n",
      "The selection starts on page 4 of 96 (499.25/106.25)\n",
      "The selection ends on page 4 of 96 (642.75/71.0)\n",
      "The selection contains\n",
      "\n",
      "Checking table 1\n",
      "The selection starts on page 3 of 96 (297.0/71.0)\n",
      "The selection ends on page 3 of 96 (579.75/71.0)\n",
      "The selection contains\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 20:51:54,714 : WordToHtmlLoggerJ : Completed document cleaning process | H | CAP |  en | .doc | Abilify Maintena~H~CAP~en.doc\n",
      "2021-05-13 20:51:54,718 : WordToHtmlLoggerJ : Preparing zip file | H | CAP |  en | .doc | Abilify Maintena~H~CAP~en.doc\n",
      "2021-05-13 20:51:55,869 : WordToHtmlLoggerJ : Zip file created: C:\\Users\\psaga\\source\\repos\\EMA\\EMA%20EPI%20PoC\\function_code\\inputblob\\Abilify Maintena~H~CAP~en~2021-05-13T15-21-14Z.zip | H | CAP |  en | .doc | Abilify Maintena~H~CAP~en.doc\n",
      "2021-05-13 20:51:55,872 : WordToHtmlLoggerJ : Uploading to Azure Storage as blob:\n",
      "\tC:\\Users\\psaga\\source\\repos\\EMA\\EMA%20EPI%20PoC\\function_code\\inputblob\\Abilify Maintena~H~CAP~en~2021-05-13T15-21-14Z.zip | H | CAP |  en | .doc | Abilify Maintena~H~CAP~en.doc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading to Azure Storage as blob:\n",
      "\tC:\\Users\\psaga\\source\\repos\\EMA\\EMA%20EPI%20PoC\\function_code\\inputblob\\Abilify Maintena~H~CAP~en~2021-05-13T15-21-14Z.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 20:52:09,002 : WordToHtmlLoggerJ : UploadedC:\\Users\\psaga\\source\\repos\\EMA\\EMA%20EPI%20PoC\\function_code\\inputblob\\Abilify Maintena~H~CAP~en~2021-05-13T15-21-14Z.zipsuccessfully | H | CAP |  en | .doc | Abilify Maintena~H~CAP~en.doc\n",
      "2021-05-13 20:52:09,004 : WordToHtmlLoggerJ : Deleting input word file: Abilify Maintena~H~CAP~en.doc | H | CAP |  en | .doc | Abilify Maintena~H~CAP~en.doc\n"
     ]
    }
   ],
   "source": [
    "wordToHtmlConvertorObj = WordToHtmlConvertor()\n",
    "wordToHtmlConvertorObj.convertWordToHTML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputZipFolderPath = \"F:\\Projects\\EMA\\Repository\\EMA EPI PoC\\\\function_code\\\\inputblob\"\n",
    "inputZipFolderPath = os.path.abspath(os.path.join('..'))\n",
    "inputZipFolderPath = os.path.join(inputZipFolderPath, 'inputblob')\n",
    "inputZipFileName = \"ABASAGLAR~H~CAP~en~2021-05-13T10-00-18Z.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fileNameQrd = 'qrd_canonical_model.csv'\n",
    "fileNameMatchRuleBook = 'ruleDict.json'\n",
    "fileNameDocumentTypeNames = 'documentTypeNames.json'\n",
    "fsMountName = '/mounted'\n",
    "\n",
    "info = inputZipFileName.split(\"~\")\n",
    "\n",
    "try:\n",
    "    medName = info[0]\n",
    "    domain = info[1]\n",
    "    procedureType = info[2]\n",
    "    languageCode = info[3]\n",
    "    timestamp = info[4]\n",
    "    timestamp = timestamp.replace(\".zip\",\"\")\n",
    "\n",
    "except Exception:\n",
    "    raise f\"Missing required info in the zip file name {inputZipFileName}\"\n",
    "\n",
    "if \"\\\\\" in os.getcwd():\n",
    "    localEnv = True\n",
    "    inputZipFolderPath = os.path.join(os.path.abspath(os.path.join('..')),inputZipFolderPath)\n",
    "    outputFolderPath = os.path.join(os.path.abspath(os.path.join('..')), 'work', f\"{domain}\", f\"{procedureType}\", f\"{medName}\", f\"{languageCode}\", f\"{timestamp}\")\n",
    "    controlFolderPath = os.path.join(os.path.abspath(os.path.join('..')),'control')\n",
    "else:\n",
    "    localEnv = False\n",
    "    inputZipFolderPath = os.path.join(f'{fsMountName}',inputZipFolderPath)\n",
    "    outputFolderPath = os.path.join(f'{fsMountName}', 'work', f\"{domain}\", f\"{procedureType}\", f\"{medName}\", f\"{languageCode}\", f\"{timestamp}\")\n",
    "    controlFolderPath = os.path.join(f'{fsMountName}','control')\n",
    "\n",
    "\n",
    "print(inputZipFileName, inputZipFolderPath, outputFolderPath, controlFolderPath)\n",
    "\n",
    "mode = 0o666\n",
    "\n",
    "if localEnv is True:\n",
    "    inputZipFolderPath = inputZipFolderPath.replace(\"/\",\"\\\\\")\n",
    "    outputFolderPath = outputFolderPath.replace(\"/\",\"\\\\\")\n",
    "    controlFolderPath = controlFolderPath.replace(\"/\",\"\\\\\")\n",
    "\n",
    "try:\n",
    "    os.makedirs(inputZipFolderPath, mode)\n",
    "    os.makedirs(outputFolderPath, mode)\n",
    "    os.makedirs(controlFolderPath, mode)\n",
    "\n",
    "except Exception:\n",
    "    print(\"Already Present\")\n",
    "    \n",
    "with zipfile.ZipFile(f'{inputZipFolderPath}/{inputZipFileName}',\"r\") as zip_ref:\n",
    "        zip_ref.extractall(outputFolderPath)\n",
    "    \n",
    "\n",
    "_,_,fileNames = next(os.walk(outputFolderPath))\n",
    "htmlFileName = [fileName for fileName in fileNames if \".htm\" in fileName][0]\n",
    "\n",
    "print(htmlFileName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseDocument(controlFolderPath, outputFolderPath, htmlFileName, fileNameQrd, fileNameMatchRuleBook, fileNameDocumentTypeNames, medName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
