{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "module_path = os.path.join(module_path, 'scripts')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vipsharm\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import sys\n",
    "from bs4 import NavigableString, BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import string\n",
    "\n",
    "from utils.config import config\n",
    "from utils.logger.logger import loggerCreator\n",
    "\n",
    "## ePI Modules\n",
    "from parse.rulebook.rulebook import StyleRulesDictionary\n",
    "\n",
    "from parse.extractor.parser import parserExtractor\n",
    "from match.matchDocument.matchDocument import MatchDocument\n",
    "from documentAnnotation.documentAnnotation import DocumentAnnotation\n",
    "from htmlDocTypePartitioner.partition import DocTypePartitioner\n",
    "from extractContentBetweenHeadings.dataBetweenHeadingsExtractor import DataBetweenHeadingsExtractor\n",
    "from fhirXmlGenerator.fhirXmlGenerator import FhirXmlGenerator\n",
    "#from fhirService.fhirService import FhirService\n",
    "from utils.logger.matchLogger import MatchLogger\n",
    "from languageInfo.documentTypeNames.documentTypeNames import DocumentTypeNames\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomString(N):\n",
    "    str_ = ''.join(random.choice(string.ascii_uppercase + string.digits \\\n",
    "            + string.ascii_lowercase) for _ in range(N))\n",
    "    return str_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#Comman Parameters\n",
    "procedureType = 'CAP'\n",
    "fileNameQrd = 'qrd_canonical_mode_CAP_NAP.csv'\n",
    "fileNameMatchRuleBook = 'ruleDict.json'\n",
    "fileNameDocumentTypeNames = 'documentTypeNames.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FolderNotFoundError(Exception):\n",
    "    pass\n",
    "\n",
    "def convertHtmlToJson(procedureType, languageCode, fileNameHtml, fileNameQrd, fileNameLog):\n",
    "    \n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    module_path = os.path.join(module_path, 'data')\n",
    "    module_path = os.path.join(module_path, 'converted_to_html')\n",
    "    module_path = os.path.join(module_path, languageCode)\n",
    "    \n",
    "\n",
    "    ## Generate output folder path\n",
    "    output_json_path = module_path.replace('converted_to_html','outputJSON')\n",
    "\n",
    "    \"\"\"\n",
    "        Check if input folder exists, else throw exception\n",
    "    \"\"\"\n",
    "    if(os.path.exists(module_path)):\n",
    "        filenames = glob.glob(os.path.join(module_path, fileNameHtml))\n",
    "\n",
    "        ## Create language specific folder in outputJSON folder if it doesn't exist\n",
    "        if(not os.path.exists(output_json_path)):\n",
    "            os.mkdir(output_json_path)\n",
    "        logger =  MatchLogger(f'Parser_{getRandomString(1)}', fileNameHtml, procedureType, languageCode, \"HTML\", fileNameLog)\n",
    " \n",
    "        styleLogger = MatchLogger(f'Style Dictionary_{getRandomString(1)}', fileNameHtml, procedureType, languageCode, \"HTML\", fileNameLog)\n",
    "     \n",
    "        styleRulesObj = StyleRulesDictionary(styleLogger,\n",
    "                                         language = languageCode,\n",
    "                                         fileName = fileNameQrd,\n",
    "                                         procedureType = procedureType)\n",
    "\n",
    "        parserObj = parserExtractor(config, logger, styleRulesObj.styleRuleDict, \n",
    "                                styleRulesObj.styleFeatureKeyList, \n",
    "                                styleRulesObj.qrd_section_headings)\n",
    "        print()\n",
    "        for input_filename in filenames:\n",
    "          #if(input_filename.find('Kalydeco II-86-PI-clean')!=-1):\n",
    "            output_filename = input_filename.replace('converted_to_html','outputJSON')\n",
    "            output_filename = output_filename.replace('.html','.json')\n",
    "            output_filename = output_filename.replace('.htm','.json')\n",
    "            print(input_filename, output_filename)\n",
    "            parserObj.createPIJsonFromHTML(input_filepath = input_filename,\n",
    "                                           output_filepath = output_filename,\n",
    "                                           img_base64_dict= parserObj.convertImgToBase64(input_filename),\n",
    "                                          )\n",
    "    else:\n",
    "        raise FolderNotFoundError(module_path + \" not found\")\n",
    "    return output_filename.split(\"\\\\\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitJson(procedureType, languageCode,fileNameJson, fileNameLog):\n",
    "    \n",
    "    styleLogger = MatchLogger(f'Style Dictionary_{getRandomString(1)}', fileNameJson, procedureType, languageCode, \"Json\", fileNameLog)\n",
    "\n",
    "    styleRulesObj = StyleRulesDictionary(styleLogger, \n",
    "                                         language = languageCode,\n",
    "                                         fileName = fileNameQrd,\n",
    "                                         procedureType = procedureType)\n",
    "\n",
    "    path_json = os.path.join(os.path.abspath(os.path.join('..')), 'data', 'outputJSON', languageCode, fileNameJson)\n",
    "    partitionLogger =  MatchLogger(f'Partition_{getRandomString(1)}', fileNameJson, procedureType, languageCode, \"Json\", fileNameLog)\n",
    "\n",
    "    partitioner = DocTypePartitioner(partitionLogger)\n",
    "    \n",
    "    partitionedJsonPaths = partitioner.partitionHtmls(styleRulesObj.qrd_section_headings, path_json)\n",
    "    \n",
    "    return partitionedJsonPaths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractAndValidateHeadings(procedureType,\n",
    "                             languageCode,\n",
    "                             documentNumber,\n",
    "                             fileNameDoc,\n",
    "                             fileNameQrd,\n",
    "                             fileNameMatchRuleBook,\n",
    "                             fileNameDocumentTypeNames,\n",
    "                             stopWordFilterLen=6,\n",
    "                             isPackageLeaflet=False,\n",
    "                             medName=None):\n",
    "    \n",
    "    if documentNumber == 0:\n",
    "        topHeadingsConsidered=4\n",
    "        bottomHeadingsConsidered=6\n",
    "    elif documentNumber == 1:\n",
    "        topHeadingsConsidered=3\n",
    "        bottomHeadingsConsidered=5\n",
    "    elif documentNumber == 2:\n",
    "        topHeadingsConsidered=5\n",
    "        bottomHeadingsConsidered=15\n",
    "    else:\n",
    "        topHeadingsConsidered=5\n",
    "        bottomHeadingsConsidered=10\n",
    "        \n",
    "    print(f\"Starting Heading Extraction For File :- {fileNameDoc}\")\n",
    "    logger = MatchLogger(f\"Heading Extraction {fileNameDoc}\", fileNameDoc, procedureType, languageCode, documentNumber, fileNameLog = os.path.join(os.path.abspath(os.path.join('..')), 'code','utils','matchLog.txt'))\n",
    "    logger.logFlowCheckpoint(\"Starting Heading Extraction\")\n",
    "    \n",
    "    stopWordlanguage = DocumentTypeNames(\n",
    "            fileNameDocumentTypeNames=fileNameDocumentTypeNames,\n",
    "            languageCode=languageCode,\n",
    "            procedureType=procedureType,\n",
    "            documentNumber=documentNumber).extractStopWordLanguage()\n",
    "    \n",
    "    matchDocObj = MatchDocument(\n",
    "             logger,\n",
    "             procedureType,\n",
    "             languageCode,\n",
    "             documentNumber,\n",
    "             fileNameDoc,\n",
    "             fileNameQrd,\n",
    "             fileNameMatchRuleBook,\n",
    "             fileNameDocumentTypeNames,\n",
    "             topHeadingsConsidered,\n",
    "             bottomHeadingsConsidered,\n",
    "             stopWordFilterLen,\n",
    "             stopWordlanguage,\n",
    "             isPackageLeaflet,\n",
    "             medName)\n",
    "    df, coll = matchDocObj.matchHtmlHeaddingsWithQrd()\n",
    "    \n",
    "    return  df, coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDocument(htmlDocPath, medName = None):\n",
    "    \n",
    "    pathComponents = htmlDocPath.split(\"\\\\\")\n",
    "    fileNameHtml = pathComponents[-1]\n",
    "    languageCode =  pathComponents[-2]\n",
    "    #procedureType = pathComponents[-3]\n",
    "    procedureType = \"CAP\"\n",
    "    print(fileNameHtml,languageCode,procedureType)\n",
    "    \n",
    "    fileNameLog = os.path.join(os.path.abspath(os.path.join('..')), 'code','utils','FinalLog.txt')\n",
    "    \n",
    "    flowLogger =  MatchLogger(\"Flow Logger HTML\", fileNameHtml, procedureType, languageCode, \"HTML\", fileNameLog)\n",
    "\n",
    "    flowLogger.logFlowCheckpoint(\"Starting HTML Conversion To Json\")\n",
    "    ###Convert Html to Json\n",
    "    fileNameJson = convertHtmlToJson(procedureType, languageCode, fileNameHtml, fileNameQrd, fileNameLog)\n",
    "    \n",
    "    flowLogger.logFlowCheckpoint(\"Completed HTML Conversion To Json\")\n",
    "\n",
    "    flowLogger.logFlowCheckpoint(\"Starting Json Split\")\n",
    "\n",
    "    ###Split Uber Json to multiple Jsons for each category.\n",
    "    partitionedJsonPaths = splitJson(procedureType, languageCode, fileNameJson, fileNameLog)\n",
    "    \n",
    "    partitionedJsonPaths = [ path.split(\"\\\\\")[-1] for path in partitionedJsonPaths]\n",
    "    #print(partitionedJsonPaths)\n",
    "    \n",
    "    flowLogger.logFlowCheckpoint(\"Completed Json Split\")\n",
    "    \n",
    "    flowLogger.logFlowCheckpoint(\"Started Processing Partitioned Jsons\")\n",
    "    \n",
    "    for index, fileNamePartitioned in enumerate(partitionedJsonPaths):\n",
    "        \n",
    "        print(index,fileNamePartitioned)\n",
    "        \n",
    "        if index == 3:\n",
    "            stopWordFilterLen = 100\n",
    "            isPackageLeaflet = True\n",
    "        else:\n",
    "            stopWordFilterLen = 6\n",
    "            isPackageLeaflet = False\n",
    "            \n",
    "        df, coll = extractAndValidateHeadings(procedureType,\n",
    "                                 languageCode,\n",
    "                                 index,\n",
    "                                 fileNamePartitioned,\n",
    "                                 fileNameQrd,\n",
    "                                 fileNameMatchRuleBook,\n",
    "                                 fileNameDocumentTypeNames,\n",
    "                                 stopWordFilterLen=stopWordFilterLen,\n",
    "                                 isPackageLeaflet=isPackageLeaflet,\n",
    "                                 medName=medName)\n",
    "        \n",
    "        \n",
    "        print(f\"Completed Heading Extraction For File\")\n",
    "        flowLogger.logFlowCheckpoint(\"Completed Heading Extraction For File\")\n",
    "        \n",
    "        print(f\"Starting Document Annotation For File :- {fileNamePartitioned}\")        \n",
    "        flowLogger.logFlowCheckpoint(\"Starting Document Annotation For File\")\n",
    "        documentAnnotationObj = DocumentAnnotation(fileNamePartitioned,'c20835db4b1b4e108828a8537ff41506','https://spor-sit.azure-api.net/pms/api/v2/',df,coll)\n",
    "        try:\n",
    "            pms_oms_annotation_data = documentAnnotationObj.processRegulatedAuthorizationForDoc()\n",
    "            print(pms_oms_annotation_data)\n",
    "        except:\n",
    "            pms_oms_annotation_data = None\n",
    "            print(\"Error Found\")\n",
    "            \n",
    "        print(f\"Completed Document Annotation\")        \n",
    "        flowLogger.logFlowCheckpoint(\"Completed Document Annotation\")\n",
    "        \n",
    "        print(f\"Starting Extracting Content Between Heading For File :- {fileNamePartitioned}\")        \n",
    "        flowLogger.logFlowCheckpoint(\"Starting Extracting Content Between Heading\")\n",
    "        \n",
    "        extractContentlogger =  MatchLogger(f'ExtractContentBetween_{index}', fileNamePartitioned, procedureType, languageCode, index, fileNameLog)\n",
    "        extractorObj = DataBetweenHeadingsExtractor(extractContentlogger, coll, languageCode)\n",
    "        dfExtractedHierRR = extractorObj.extractContentBetweenHeadings(fileNamePartitioned)\n",
    "        \n",
    "        print(f\"Completed Extracting Content Between Heading\")        \n",
    "        flowLogger.logFlowCheckpoint(\"Completed Extracting Content Between Heading\")\n",
    "        \n",
    "        xmlLogger =  MatchLogger(f'XmlGeneration_{index}', fileNamePartitioned, procedureType, languageCode, index, fileNameLog)\n",
    "        fhirXmlGeneratorObj = FhirXmlGenerator(xmlLogger, pms_oms_annotation_data)\n",
    "        fileNameXml = fileNamePartitioned.replace('.json','.xml')\n",
    "        generatedXml = fhirXmlGeneratorObj.generateXml(dfExtractedHierRR,  fileNameXml)\n",
    "        \n",
    "        #fhirServiceObj = FhirService(generatedXml)\n",
    "        #fhirServiceObj.submitFhirXml()\n",
    "        print(f\"Created XML File For :- {fileNamePartitioned}\")        \n",
    "\n",
    "        #return df,coll,dfExtractedHierRR\n",
    "    \n",
    "    flowLogger.logFlowCheckpoint(\"Completed Processing Partitioned Jsons\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToInt(x):\n",
    "    try:\n",
    "        return str(int(x))\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "\n",
    "def convertCollectionToDataFrame(collection):\n",
    "\n",
    "    dfExtractedHier = pd.DataFrame(collection)\n",
    "    dfExtractedHier['parent_id'] = dfExtractedHier['parent_id'].apply(\n",
    "        lambda x: convertToInt(x))\n",
    "    dfExtractedHier['id'] = dfExtractedHier['id'].apply(\n",
    "        lambda x: convertToInt(x))\n",
    "\n",
    "    return dfExtractedHier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseDocument(\"F:\\Projects\\EMA\\Repository\\EMA EPI PoC\\function_code\\data\\converted_to_html\\en\\Abasaglar-h-2835-en.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
